SED and DOA joint training using CRNN 
OUTPUTS:
1OUTPUT FOR SED(Sound Event Detection)-> detects and classifies event
1OUTPUT FOR DOA(Direction of Arrival)->detects location of said event
OR
1 ACCDOA representation 

INPUTS:
1INPUT: A tetrahedral microphone array (MIC) 
1INPUT: A first-order Ambisonics (FOA)

>>Ensembles???
>>Transformers better performance??
Transformer aims at transforming an input feature
sequence into its corresponding output sequence
It comprises of encoder-decoder architecture and employs a self-attention mechanism
with multi-head attention (MHSA).
>>RD3Net for classification->less complex and heavy
>>GRU modules are replaced by two Conformer blocks
>>EINV-2
>>

>>DATA AUGMENTATION

>>ACCDOA representation system

CNN acts as the feature extractor

Baseline: SELDNet-> CRNN receiving multichannel log-mel spectrograms as inputs,
 together with acoustic intensity vectors for
the FOA dataset, and generalized cross-correlation (GCC-PHAT)
sequences for the MIC dataset, added as extra channels

///// DCASE 2021 TASK 3: SELD SYSTEM BASED ON RESNET AND RANDOM SEGMENT
AUGMENTATION
APPROACH: Resnet architectures are used in the
task as main network for SELD, and GRU is used after the Resnet
for catching temporal relationship of acoustic features. Moreover,
a data augmentation method called random segment augmentation
is adopted during training.
uses the Resnet network borrowed from machine vision
ARCHITECTURE:
ResNet
DATA AUGMENTATION:
Slice reorganisation

 Sound recognition is an image
recognition process. Therefore, many network architectures used
for machine vision can be used for reference, and sound is a signal
with temporal context, so many networks used for natural language
processing can also be used for reference

INPUTS: Log-mel spectrum and GCC-PHAT

data augmentation: Data augmentation is an effective way to improve the model generalization and prevent overfitting problem
when training data not enough, create more with data enhancement method
Resnet-34 network
GCC-PHAT: spatial input
specAugment: data enhancement method

///// SSELDNET: A FULLY END-TO-END SAMPLE-LEVEL FRAMEWORK FOR SOUND EVENT
LOCALIZATION AND DETECTION
APPROACH: Learning directly from raw audio with a framework,
data augmentation in time domain
audio-based end-to-end SELD system called SSELDnet, which does not depend on the human extracted features,
ARCHITECTURE:
SampleCNN->much smaller kernel for better performance
SE-ResNet Module->for the classification, convolution part of the CNN
2 Conformer blocks->combination of convolution and transformer

data augmentation:
SFR (Sound Field Rotation)
Time Masking
Random Audio Equalization

INPUT: MIC (log-Mel Spectrum and GCC-PHAT) or FOA(log-Mel spectrum and Intensity Vector)
OUTPUT: SED and DOA simultaneously
Time Masking (TM) and Random Audio Equalization (RAE)
GRU modules are replaced by two Conformer blocks
We
believe SED and DOA may have some common features that are
better preserved in raw audio form, so no mel-spectrogram extraction needed

Using large kernel CNN will not improve the generalization ability.

limited set of augmentation methods that can be reliably
applied to our system to ensure the DOA estimation will not be negatively impacted.

For SED
task, there are two evaluation metrics, including location-dependent
F-score (F≤T)and Error-Rate (ER≤T), which only consider predicted events under a certain threshold T
, where in this challenge
T = 20. For DOA task, classification-dependent Localization Error (LECD) and Localization Recall (LRCD) are evaluated, where
LECD represents the average angular distance between ground
truth and prediction, and LRCD stands for true positive rate of how
many locations estimates are detected in a class.

// A COMBINATION OF VARIOUS NEURAL NETWORKS FOR SOUND EVENT
LOCALIZATION AND DETECTION
APPROACH: Combine various network layers, data augmentation
and training and post-processing techniques
ARCHITECTURE:
CNN and max-pooliong layers (as many as the label frames)
Conformer for SED 
2 bidirectional GRU for DOA
DATA AUGMENTATION:
Frequency masking
FOA domain spatial augmentation
Multiply magnitude by random constant
OTHER TECHNIQUES:
-Adaptive gradient clipping
-Ensembling
-Class-wise dynamic threshholds

First-order ambisonic (FOA) formatted audio files have seven
channels: four log-mel spectrograms and three intensity vectors

CNN and Max pool
GRU and Conformer for output layer

DOA in cartesian form
Sigmoid for SED, tangent for DOA

Masking for data augmentation>>detecting start-points and end-points of sound events
are critical. Time-masking makes it harder to predict exact timing
of sound events and this might have affected models in a negative
way.

Ensemble methods 

//SOUND EVENT LOCALIZATION AND DETECTION USING CROSS-MODAL 
ATTENTION AND PARAMETER SHARING FOR DCASE2021 CHALLENGE
APPROACH:
Parameter sharing, trying to find association between SED and DOA predictions

ARCHITECTURE:
CNN and RNN in parallel to extract features for sed and doa in parallel
feature sharing
->TRANSFORMER DECODER LAYERS
->CMA for multitasking learning->to learn associations between SED and DOA

DATA AUGMENTATION:
-mixup (sum up weighted audio clips)
-frequency masking (to input spectograms via specAugment)
-channel rotation (exchange audios by channel)

The outputs of the previous FC networks are concatenated to the input of the current FC network
to predict an event different from the previous one

outputs three predictions for SED and three for DOA 
to detect up to three overlapping events

>>CNN ouperformed LSTM

//SOUND EVENT LOCALIZATION AND DETECTION USING SQUEEZE-EXCITATION
RESIDUAL CNNS
APPROACH:
Study the squeeze-excitation technique in the convolutional part
ARCHITECTURE:
-scSE (concurrent spatial and channel SQUEEZE and EXCITATION)
-Replace cnn of SELDnet with Conv-StandardPOST
Conv-StandardPOST includes the scSE module in it
NO DATA AUGMENTATION

10 channels (GCC) are obtained and with the first-order ambisonics
(FOA), 7 channels (Intensity vector).

 implementation
of the scSE block

a learning rate decay: if the performance of the system is not improved
within 15 epochs, the learning rate decreases by a factor of 0.5

>>NO IMPROVEMENT FOR FOA

//SPECTROTEMPORALLY-ALIGNED FEATURES
FOR POLYPHONIC SOUND EVENT LOCALIZATION AND DETECTION

APPROACH:
spatial cue-augmented log-spectrogram (SALSA)with exact time-frequency mapping between the signal
power and the source direction-of-arrival

ARCHITECTURE:
SALSA features -> SELD encoder: ResNet22
->SELD decoder: LSTM, bidirectional GRU, MHSA and combine in ensembles

DATA AUGMENTATION:
-time masking
-frequency masking

APPROACHES MENTIONED:
While sound event detection mainly relies on time-frequency
patterns to distinguish different sound classes, direction-of-arrival
estimation uses magnitude or phase differences between microphones to estimate source directions
>combat this by: training separately SED and DOA, then use the SED outputs as masks for DOA predictions

Event Independent Network (EIN)-> uses parameter sharing
->even more optimised->MHSA

Polyphonic
SELD refers to cases where there are multiple sound events overlapping in time

Hirvonen first formulated SELD as a
multi-class classification task where the number of output classes
is equal to the number of DOAs multiplied by the number of
sound classes 

 two-stage strategy by training separate SED and DOA models , then using the SED outputs
as masks to select DOA outputs

//SELF-ATTENTION MECHANISM FOR SOUND EVENT LOCALIZATION AND DETECTION
APPROACH:
Use of tranformer network M2M-AST to lower the dependencies of the CNN
ARCHITECTURE:
pure tranformer M2M-AST
NO DATA AUGMENTATION

Transformers replacing CNNs
Due to the success of CNNs in
image understanding, CNNs have also been used in other pattern
recognition fields

//MULTI-SCALE NETWORK FOR SOUND EVENT LOCALIZATION AND DETECTION

APPROACH:
 end-to-end trained deep network for
jointly detecting, classifying, and localizing the acoustic target
classes using the FOA data
ARCHITECTURE:
NAS layers to process inputs->
->parsing blocks->
->MHSA to predict output

DATA AUGMENTATION:
-wav mixing
-rotation augmentation


Recently, however, end-to-end trained
convolutional recurrent neural networks (CRNN) have been shown
to exceed the performance of traditional algorithms for sound event
detection and localization

DUe to polyphony in input->> design
the network to operate at multiple time/frequency scales throughout

Training with:
-constant-Q
-logmel spectral

Data augmentation: rotation augmentation-> first transform the coordinates 
from (azimuth, elevation) to cartesian (x, y, z). We then apply a random rotation matrix to the truth
and the x, y, z channels of the FOA

Network:
-Convolutional network feature extractor->
->Multi-Head-Self-Attention (MHSA)

To facilitate learning from signals with potentially varying spectral
characteristics we use NASnet-like convolution modules, termed
NAS blocks in Figure 1, along with convolution modules inspired
by PSPNet
We anticipate that these multi-scale modules will enable the CNN
to extract features at varying scales in the data, allowing for potentially better generalization
to acoustic targets with varying spectral characteristics and bandwidths.

//A DATASET OF DYNAMIC REVERBERANT SOUND SCENES WITH DIRECTIONAL
INTERFERERS FOR SOUND EVENT LOCALIZATION AND DETECTION
APPROACH:
 investigate the individual and combined effects of ambient noise, interferers, and reverberation,
in the performance of the baseline on different versions of the
dataset excluding or including combinations of these factors

This report: uses activity-coupled cartesian direction of arrival representation (ACCDOA) 
which unifies the SED and SSL losses into a single homogeneous regression loss,
simplifying the overall architecture 

anechoic array to get rid of reverberation
Reverberation increases error

//ENSEMBLE OF ACCDOA- AND EINV2-BASED SYSTEMS WITH D3NETS AND IMPULSE
RESPONSE SIMULATION FOR SOUND EVENT LOCALIZATION AND DETECTION

APPROACH:
perform model ensembles by averaging outputs of several systems 
trained with different conditions.
Also propose impulse response simulation (IRS), which generates simulated multi-channel signals 
by convolving simulated room impulse responses (RIRs) with source signals extracted 
from the original dataset

ARCHITECTURE:
EINV-2 based model (see 6th report mentioning EIN networks)
Variants of D3Net

DATA AUGMENTATION:
-equalized mixure
-rotation (spatial augmentation method )
-specAugment for multichannel
-impulse response simulation (IRS): generate simulated multichannel signals

model is trained to
minimize the Euclidean distance between the estimated and target
coordinates in the ACCDOA representation.

MSE

IPDs are used as frame-wise features
Short Time Fourier Transfor (STFT)

D3Net architecture for separation

EINV2-BASED: Track-wise output->each output has several tracks, each track belong in a SE class

model enseble: averaging outputs of several
models trained with different conditions such as input features,
training folds, and model architectures

//SOUND EVENT LOCALIZATION AND DETECTION BASED ON CRNN USING ADAPTIVE
HYBRID CONVOLUTION AND MULTI-SCALE FEATURE EXTRACTOR
APPROACH:
 method based
on Adaptive Hybrid Convolution (AHConv) and multi-scale feature
extractor
ALSO explored multi-scale feature extractor which can integrate 
information from very local to exponentially large receptive field within the
block.

ARCHITECTURE:
AHConv
Multi-Scale feature extractor (combination of DenseNet and dilated convolution), 
captures the longer temporal context 
information than the conventional convolutions
Adaptive attention block,can also promote the robustness when 
a single branch is disturbed by ambient noise

NO DATA AUGMENTATION

BiGRU:  learn the temporal
context information
DesNet
dilated convolution
multi-scale extractor

Adaptive attention block:That can enhance the important features and weaken the less
important features

//THE HITACHI DCASE 2021 TASK 3 SYSTEM: HANDLING DIRECTIVE INTERFERENCE
WITH SELF ATTENTION LAYERS

APPROACH:
a single-stage system that
employs the transformer encoder (i.e., self-attention layers) as a
core idea

ARCHITECTURE:
Transformer encoder only(NOT DECODER) with Residual, to model the locations and
the classes of the sound events

DATA AUGMENTATION:
-Time masking
-Frequency Masking
-Time Wraping
-Speed perturbation

specAugment

 intensity
vectors carry the acoustical energy direction of a given sound wave
-> calculate in STSFT

Transformer

SpeechBrain toolkit on PyTorch

The use of Residual connections in a SELDnet model improves 
localization recall
 
//DATA AUGMENTATION AND CLASS-BASED ENSEMBLED CNN-CONFORMER
NETWORKS FOR SOUND EVENT LOCALIZATION AND DETECTION

APPROACH:
We introduce Conformer block into the baseline system 
to make better use of temporal context information for the SELD task
Class ensemlbling

ARCHITECTURE:
bidirectional-GRU replaced by Conformer blocks
MHSA

Conformer module combines convolutional networks and Transformers

The 17 feature maps are fed into the CNN
 blocks firstly to extract high-level features.
 
DATA AUGMENTATION:
-Audio channel swap
-Speed perturbation
-Time/frequency masking

Model enseble method: a collection (sum) of all thecoordinates predicted from all the models for one class