batches*(10x300x64) is (MIC channels, sequence length (number of STFT frames), mel-bands)

n_features = n_channels
freq = n_mel_bins


TO DOO:::

1)Parameter sharing
SED + DOA separate

2)Data augmentation

3)LSTM ->>in progress DONE 

2/3/2022

Check resnet18 dimension problem
Write eidiko thema report
Copied cudnn64_7.dll on c:/program files/nvidia/cuda/bin->>>cuda recognised on dummy program


3/3/2022
Check Park's report SELF-ATTENTION MECHANISM FOR SOUND EVENT LOCALIZATION AND DETECTION
for self-attention

AAAAnnyyywaysss

IDEA:combined Conformer and ResNet--->>>>eNSEMBLE-LIKE SYSTEM??
replace the GRU block
with a time-frequency RNN (TFRNN) block (read Shimada's paper)

Read A.Politis report(DCASE baseline report):
 The DOA methods in literature
can be broadly categorized into parametric- and deep neural
network (DNN)-based approaches(SEE https://www.aane.in/research/computational-audio-scene-analysis-casa/sound-event-localization-and-tracking
FOR MORE)

DNN based methods for DOA for static sources better than parametric

 microphone arrays with
full azimuth and elevation coverage, such as spherical microphone arrays, rely strongly on the directionality of the sensors
to capture spatial information, this reflects mainly in the magnitude differences between channels. Motivated by this fact
we proposed to use both the magnitude and phase component
of the spectrogram as input features in [25]. Thus making the
DOA estimation method [25] generic to array configuration
by avoiding method-specific feature extractions

 The problem is further extended for the polyphonic
SELD task if the SED and DOA estimation are done separately, resulting in the data association problem between
the recognized sound events and the estimated DOAs [13].
One solution to the data association problem is to jointly
predict the SED and DOA. In this regard, to the best of
the authorsâ€™ knowledge, [20] is the only DNN-based method
which performs SELD. Other works combining SED and
parametric DOA estimation include [

e number of existing SELD methods is limited [6, 13, 20, 52, 53], with only one published DNN-based
approach

RWTAW PPOTAMIANO GIA TO EXTRACTION (MEL BANDS, FFT .... NUMBER OF BINS?? )


5/02/2022

TO DO:: MSE loss instead of ACCDOA

Lowered batch size to get rid of OEM in GPU
Got new error (yay)::
Internal: Blas GEMM launch failed

->>import keras.backend.tensorflow_backend as KTF
import tensorflow as tf
config = tf.ConfigProto()
config.gpu_options.allow_growth=True   
sess = tf.Session(config=config)

KTF.set_session(sess)

NOT WORKING
TRY IT ON UBUNTU WITH SMALLER BATCH SIZE

6/02/2022

Continue conformer block on ubuntu
also try resnet on GPU ubuntu

If nothing works, ill try pytorch in future, need to learn pytorch first

TO DO: MAKE CONFORMER IMPLEMENTATION
SOURCES:
Conformer: Convolution-augmented Transformer for Speech Recognition
Attention is all you need (for the transformer)
https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Ko_127_t3.pdf
https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Huang_24_t3.pdf
https://dcase.community/documents/challenge2021/technical_reports/DCASE2021_Zhang_67_t3.pd

8/3/2022
Tensorflow addons for MultiHeadAttention
https://github.com/tensorflow/addons/tree/v0.15.0

9/3/2022
Encoder->Embeddings->Attention->Decoder
Encoder: One-hot of 12 bits, for 12 classes

Data augmentation:https://github.com/thomeou/SALSA/blob/master/utilities/transforms.py
TO DO:
check positional encoding->how it is embedded in MHSA Module
try and run

18/3/2022
going with Zhang

keep in mind:::: downloaded cudnn 10.0, 7.65 might try to replace previous cudnn NOT YET!!!!!!
trying to run test on conformer, dimensions and subsampling layer idea based on Zhang

RWTAW PPOTAMIANO GIA TO EXTRACTION (MEL BANDS, FFT .... NUMBER OF BINS?? )

19/3/2022
PRENORM in FFN
The original Transformer uses post-norm residual units
(POSTNORM), where layer normalization occurs
after the sublayer and residual addition. However,
Chen et al. (2018) found that pre-norm residual
units (PRENORM), where layer normalization occurs immediately before the sublayer,
https://arxiv.org/pdf/1910.05895.pdf
https://github.com/tnq177/transformers_without_tears/tree/25026061979916afb193274438f7097945acf9bc

GLU: https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L551

installed tensorflow 2.0
CHECK LAYER NORMALIZATION, ONLY EXISTS IN TF.KERAS. 
FOUND THIS FOR KERAS: https://github.com/CyberZHG/keras-layer-normalization
CHANGED import tensorflow.keras backend to just keras backend in the layer-normalization.py code from the above  github

FOR CONV MODULE in CONFORMER:
To study the effect of kernel sizes in the depthwise convolution, we sweep the kernel size in {3, 7, 17, 32, 65} of the large
model, using the same kernel size for all layers. We find that the
performance improves with larger kernel sizes till kernel sizes
17 and 32. 32 IS BEST PERFORMING(file:///C:/Users/pouli/Documents/Mathimata/Eidiko_Thema/INFO/online_material/conformer_speech_recognition.pdf)
ALSO to this:
Zhang used 31 size kernel for their depthwise convolution

debugging mhsa :P

21/3/2022

Debugging

TODO: Read transformers xl, on bus
Read more on dimensions

DATA AUGM:https://github.com/thomeou/SALSA/blob/90562a236117f4810a4a821b6ae165d1255650eb/utilities/transforms.py

 Datagen_mode: dev, nb_files: 400, nb_classes:12
        nb_frames_file: 3000, feat_len: 64, nb_ch: 10, label_len:48

        Dataset: mic, split: [1, 2, 3, 4]
        batch_size: 128, feat_seq_len: 300, label_seq_len: 60, shuffle: True
        Total batches in dataset: 31
	Data in (128 (batch_size), 10 (nb_channels), 300 (seq_len), 64 (feat_len))

d_model: number of feature maps
nomizww d_model = channels*features = 256*2 = 512
nomiza arxika oti d_model = num_heads*depth alla de kserw to eida se ena github kai avto...

SCALED ATTENTION CHECK DIMENSIONS 

22/3/2022

pre-processing: ypodeigmatolipsia: meiwsh syxnwthtas
				Hamming window size
				
pos encoding has 3 dimensions in https://github.com/lucidrains/conformer/blob/0e91f0323089cc00fbde3e112a984d94d0af09b8/conformer/conformer.py#L10
but mine has 4
https://github.com/sooftware/conformer/blob/9318418cef5b516cd094c0ce2a06b80309938c70/conformer/attention.py has 3 too
but changes it to 4 through dense layer, before multiplying q*pos_emb (likewise mine)

CHANGED base_layer.py line 597 from ._kera_shape to .shape
DOWNLOADED TENSORFLOW 2.2.0
CHANGED https://stackoverflow.com/questions/60581677/experimental-list-devices-attribute-missing-in-tensorflow-core-api-v2-config
devices = tf.config.list_logical_devices()

_LOCAL_DEVICES = [x.name for x in devices]

DOWNLOADED TENSORFLOW 2.3.0
CHANGED https://github.com/tensorflow/tensorflow/issues/38589 
from tensorflow.python.framework import tensor_util
def is_tensor(x):                                                                                                                                                      
    return tensor_util.is_tensor(x)
	
DOWNLOADE TENSORFLOW 2.0.0
I cant take these conflicts anymore..... cant even code where is the fun

if cannot work until friday, i move on with data augmentation->pitch shift
(des kai diplwmatikh 2019 exei gia pitch shift)

tuple has no attribute _keras_shape::::
TURNED ALL MultiHeadAttention LAYERS TO FUNCTIONS

TO DO : -look into convmodule
		-data augmentation start sth
		
23/3/2022

Look into Ko's github: https://github.com/IRIS-AUDIO/SELD/blob/669ead73ce1e0db7bafef96d9f4037f9cf2cd0b7/modules.py#L1
https://github.com/TensorSpeech/TensorFlowASR/blob/main/tensorflow_asr/models/encoders/conformer.py
	
	# GLU Part
        conv_1, conv_2 = tf.split(conv, 2, axis=-1)
        conv_2 = tf.keras.activations.sigmoid(conv_2)
        conv = conv_1 * conv_2
		
INSTALLED TENSORFLOW 2.3.0 TO RUN DEPTHWISECONV2D LAYER

input output dimensions of conformer same(?)
will use permute and reshappe to make it into (60,512)

 Keras 2.3.1, which supports TensorFlow 2.x and 1.x, and is the latest real releases of Keras. You can also install Keras 2.2.4 which only supports TensorFlow 1.x.
 (https://stackoverflow.com/questions/62690377/tensorflow-compatibility-with-keras)
 
Tensorflow 2.3.0 gives another error i cannot solve
downgraded to 2.2.0
if i have to change version one more time i kill myself

When all versions fail try to uninstall cleanly
!pip uninstall keras -y
!pip uninstall keras-nightly -y
!pip uninstall keras-Preprocessing -y
!pip uninstall keras-vis -y
!pip uninstall tensorflow -y
(https://github.com/fizyr/keras-retinanet/issues/1538)

ON convolutions (depthwise, pointwise in pytorch though): https://github.com/sooftware/conformer/blob/9318418cef5b516cd094c0ce2a06b80309938c70/conformer/convolution.py#L24
CHECK KO'S GITHUB->his conformer input shape (batch, time, feat), but mine (influenced by Zhang) is (None, 256, 60, 2)=(batch, time, ??)
might revisit this in future, now next in list (if conformer works):
	TO DO:: Data augmentation technique
			--later, look into doa
			and try and implement idea of 3 overlapping sounds (where i run the input in 3 different frequencies)

			SUBSTITUTE Bi-GRU, no need just keep the conformers x2 (Zhang)

DEGRADED TO TENSORFLOW 1.15.0 BECAUSE MODEL COULD NOT RUN, tensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable _AnonymousVar50 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar50/class tensorflow::Var does not exist.
         [[node conformer_1/layer_normalization_5/mul/ReadVariableOp (defined at C:\Users\pouli\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py:1751) ]] [Op:__inference_keras_scratch_graph_13788]

Function call stack:
keras_scratch_graph

mAYBE STH TO DO WITH k.SET_SESSION IN KERAS_MODELS???? NOooooo.... ITS THE CONFORMER
Turned Conformer class into function 

ALSO:
CHECK RESNETS, why last dense layer is 12???
!!!!!!!!!!! for resnet-34 last dense layer (512, 12, 12) NOT WORKING while resnet18 (256, 60, 12)
FIXED: resnet34 runs with 64 batch size (or lower) only!

K.set_session(session) in comments, tensorflow 2.x does not use it

LOAD ALREADY TRAINED MODELS: 
	from tensorflow.python.keras.models import load_model
	model = load_model(self.filename)
	
	Where self.filename is the name of the .h5 file in folder models (i believe)
	
UPGRADED TensorFlow 2.0.0

Added::
physical_devices = tf.config.experimental.list_physical_devices('GPU')
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)
	
on seld.py because gpu memory error again

TO DO::		-run on UBUNTU GPU to check if it still runs out of memory
			-Data augmentation technique->SpecAugment??Pitch shift
			-SUBSTITUTE Bi-GRU, no need just keep the conformers x2 (Zhang)
			--later, look into doa
			and try and implement idea of 3 overlapping sounds (where i run the input in 3 different frequencies)		
			-PLAY WITH PARAMETERS!!! (batch_size(already seen), frequency, window length)
	
24/3/2022

According to https://www.tensorflow.org/install/source?fbclid=IwAR0VECKFjEc6stp8rYYNjlV24usNzAr-kOSbDVblmsxqn4IKn6pNzrUUoMk#tensorflow_1x
i have cuda 11.0 so i need cudnn 8.0
Downloaded cudnn 8.0 and updated tensorflow 2.4.0

downgraded to tensorflow 2.0.0, 2.4.0 gives _Tensorlike error

Trying to modify all tf.[..] functions from models.py into keras layers
to be able to run conformer without the Resource localhost/_AnonymousVar50/class tensorflow::Var does not exist. Error

Put all tf.config gpu in comments in keras_models
UNDONE: put all print(spec_cnn) in comments in models.py and keras_models.py, cause not recognized by tensorflow version 2.4.0 but can be printed now that i have 2.0.0

TODO::::::: UBUNTUUUU, pitch shift data augm, new report, also try and use is_accdoa==false
LOOK INTO sel.py AND TRY TO UNDERSTAND HOW IT WORKS!!!!!!! ->>>>>>>>ASAPPPPPP, need for implementing a data augm technique
DOING:::::: Trying to debug conformer , getting error Resource localhost/_AnonymousVar50/class tensorflow::Var does not exist. Error

MUST SEE::(for sed and doa discrimination) https://github.com/thomeou/SALSA/blob/master/models/decoders.py
Also in the above git, she uses utilities to write experiment results in a folder-->>>>> i can use that
ALSO check the online_material folder and read 

TODO later(elafri diavasmataki):Shimadas paper  EINV2-based EINV2-D3Net approaches my idea of using 3 different outputs for the 3 overlapping sounds->>> check it out


model.fit_generator (seld.py)->>rtains the model, is the one that prints Epoch ?/5

OOM when allocating tensor with shape[7864320,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
         [[{{node conformer_1/dense_5/MatMul}}]]
		 maybe matmul problem->>>try and replace it with k.backend
		 
		 
25/3/2022

Ubuntu did not work
probably need to replace tf.matmul cause thats where it always crashes
Also need to fix gpu recognition->>download other cudnn versions

Now need to look into data manipulation to understand how to implement data augm

hop_len parameter has effect on the hamming window length->> can tinker

feature_label_resolution = int(params['label_hop_len_s'] // params['hop_len_s'])
params['feature_sequence_length'] = params['label_sequence_length'] * feature_label_resolution
feature_sequence_length=600

feat is data_in and labels is data_out(what we try to predict):
feat_shape = (self._batch_size, self._nb_ch, self._feature_seq_len, self._nb_mel_bins)
label_shape = (self._batch_size, self._label_seq_len, self._nb_classes*3)

Maybe i need to modify the cs_data_generator.py to add data augmentation methods

what is a generator (needed in fit_generator to train model):https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do

FOR DATA AUGM::
https://github.com/TensorSpeech/TensorFlowASR/tree/caee2e79d0eb9d1a997f5df8e2ba3ee7f0a1bcae/tensorflow_asr/augmentations
https://github.com/thomeou/SALSA/blob/master/utilities/transforms.py
https://github.com/IRIS-AUDIO/SELD/blob/669ead73ce1e0db7bafef96d9f4037f9cf2cd0b7/transforms.py

parameter sharing: see Lees report, and https://arxiv.org/pdf/2010.13092.pdf

on doa ksexwrismo(from https://github.com/yinkalario/Two-Stage-Polyphonic-Sound-Event-Detection-and-Localization/blob/master/models/CRNNs.py)::
(x, _) = self.gru(x)
event_output = torch.sigmoid(self.event_fc(x))
azimuth_output = self.azimuth_fc(x)
elevation_output = self.elevation_fc(x) 


https://arxiv.org/abs/1904.08779::
SpecAugment is actually 3 different augmentation techniques of the log mel spectrogram: time warping, frequency masking and time masking


TODO:: --Implement masking data augmentation	
		-Debug conformer (change tf.matmul)
		-Fix gpu version compatibility to finally run gpu
		
Probably will need to make the data augmentations in the cls_feature_class file after all
and run batch_feature_extraction.py everytime

FOR THE REPORT::: Resnet18 results, data augmentation idea and conformer idea

did RandomShiftUpDown and run batch_feature_extraction.py
It takes 3d array while cls_feature_class takes 2d-> need to add reshape-->>DONE
also check how to return a flag after randomshift to know if it has been augmented->>DONE

check feature and label relationship->how data augm afFects it

26/3/2022

TODO:::-GPU	
		-Conformer debug
		-data augm debug
		-check baaseline with not an accdoa output->ACTUALLY DONT, its already in the paper "A DATASET OF DYNAMIC REVERBERANT SOUND SCENES WITH DIRECTIONAL
INTERFERERS FOR SOUND EVENT LOCALIZATION AND DETECTION"
		
in cls_generate_data printed in generate() the length of feat and label:10 2
and in seld.py after prediction, pred length is 2 (as expected like the label)

spectrogram shape: [_max_feat_frames, nb_mel_bins, nb_channels], each channel is an stft

shape needed for augmentation techniques (by Ngyuen): [channels, time_steps, features]
by Ko: (batch, time, freq, chan)->> same as data_in (256, 300, 64(mel_bins or frequency), 10(channels)) (i believe)

Nguyen used 32 batch size->she had same problem as me, OEM

cls_data_generator(generates data format for training) or cls_feature_class(when features are exrtracted and normalized)?? where should data augm take place?
i decide to do it in cls_feature_class , saving the augmented audio files in the seld_feat_label folder locally
Later when dataGenerator is called, it reads from the folder and takes the extra files into account

!!!I could do it in cls_data_generator to not save them in folder and rather save them in variables->similar to Nguyen

From Ko's report: only apply frequency masking, cause time masking requires determining start and end of a sound event->difficult
Simply scaling models
leads to improvement only on train dataset, and not on validation
nor test datasets.

where in the pipeline should data augmentation be used:https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/
basically, offline and online data augmentation
Ko and Nguyen use online (i believe)
Nguyen pipeline: database>dataloader>datamodule

For the SELD task, some augmentation operations modify the input
data as well as the SED or DOA labels

CONCERNS:-data augmentations that have effect on labels as well
		-online or offline data augmentation
		
there is a limited set of augmentation methods that can be reliably
applied to our system to ensure the DOA estimation will not be negatively impacted. (Huangs report)
8 transformations for MIC and 16 for FOA to keep the spatial info unchanged		

Huang:
For the data augmentation, all of the data augmentation tricks are applied ONLINE, which means the system randomly
chose some hyper-parameters and then apply augmentation before
the data is fed into the network. This operation will efficiently reduce the memory usage

MAYBE DO ONLINE DATA AUGMENTATION AFTER ALL-> check cls_data_generator in generate()

27/3/2022

offline daata augm:
https://arxiv.org/pdf/2010.06422.pdf


https://ieeexplore.ieee.org/document/7472671

!!!!!!!!(from Nguyen) We train the majority of these
models using the SEDXYZ output format and some models with the
ACCDOA output format. Since these two output formats are both
class-wise format, they can be easily aggregated into ensembles
using mean operation. The disadvantage of the class-wise output
format is that they cannot resolve overlapping same-class events,
which accounts for 10.45 % of the total frames in the DCASE 2021
SELD dataset. We chose to use class-wise outputs for the ease
of ensemble.

TODO:::: Look into different output formats
		-gpu
		-conformer
		-data augm
		-Gaussian Mixture Model: Ko has it in his github look into it, and https://www.e-ce.uth.gr/wp-content/uploads/formidable/59/Athanasiou_Ioannis.pdf, https://arxiv.org/pdf/2101.02919.pdf
		also from sklearn.mixture import GaussianMixture
		-Ensembling
		-parameter sharing (like Lee's report)(if i have time)
		
DOING::: offline data augm, later try online for masking, shifting and magnitude change.
		 also debug conformer 
		 
in data_generator, features have shape(when running batch_normalization): [3000, 640] = [seq_len(?), nb_channels(10)*nb_mel_bins(64)]
1: fold6_room1_mix002.wav, (3000, 640)

DONE: offline data augmentation
UP NEXT: conformer debugging and online data augmentation(done but not tested)

28/3/2022

Zhang report: masks are generated in each batch, and no new labels will be added
Huang report:Due to the convolution layers have the property to capture the fine-grained
local features while the transformer is capable of learning long sequence dependencies, Conformer was considered powerful
enough to extract both local and global features of audios. In our experiment, we used 2 Conformer blocks after feature embedding

31/3/2022

Kos	data_loader: 
# shape of x: [batch, label_window_size*5, freq, chan]
# shape of y: [batch, label_window_size, n_classes*4]

Kos modules test on conformer_encoder_block:
input_shape = 32, 100, 64 # batch, time, feat
output_shape = 32, 100, 64 # batch, time, feat

Zhang (from which i try to copy) says for conformer:
The output activation from CNN is further reshaped to a 60
frame sequence of length 512 feature vectors and fed to Conformer
block,

feat, time, freq 

1/4/2022

might need to reshape spec_cnn from (256, 60, 2)->(60,512) in the conformer input
since i think that it needs shape (B, T, dim(=C*F)) (look at sooftware github, input for conformer is (B, T, dim))

Kos implementation use tf.Conv1D while i use keras.Conv1D
the difference is that in tf.Conv1D it reshapes the last dimension so (B, C, dim) -> (B, C, 2*dim)
while keras reshapes the first like: (B, C, dim )->(B, 2*C, dim)
thats why in the split for GLU, i got dimension error cause Ko was spliting the last augmented dimension
while mine was the first that was augmented
-->>so ill need to change the axis split from -1 to 1 to split according to the 1st dimension
look at sooftware, who is using nn.Conv1D which also augments the 1st dimension->he uses glu on axis=1


conformer debuged!! batch size = 8 on quick dev run

trying to run it on gpu gives GMEM Blass error->>>cudnn and cuda compatibility issues
so i try and update tensorflow 2.4.0 and get rid of Tensorlike error (see notes 24/3/2022)
updated keras 2.4.0

i can either have tensorflow 2.4.0 with cuda 11.0 and cudnn 8.0->>need to donwload cudnn 8.0

windows requires for tensorflow-gpu to also be installed after 2.x version of tensorflow exists

changed all keras. to tensorflow.keras in keras_models.py and models.py
also all conv2d in convolutionmodule to conv1d and axis=-1 again
and used LayerNormalization from tensorflow layers
because of this, i needed to add a tf.transpose layer (in keras_models.py before the forst conv layer in conv subsampling)
cause tensorflow operates on the last dimensions while keras on the first

tf.config.run_functions_eagerly(True)->not doin anything
downgraded tensorflow, tensorflow-gpu to 2.0.0 :(
TO DO:::Fix the damn gpu problem!!!!!!!!

3/4/2022


i have changed keras.backend.set_image_data_format('channels_last') form 'channel_first'

error in line 185 in seld.py->model.save(model_name):::	raise NotImplementedError('Layers with arguments in `__init__` must '
NotImplementedError: Layers with arguments in `__init__` must override `get_config`.

->>>to do: read Kos github and the below site to understand how to save and train in tensorflow:
https://www.tensorflow.org/guide/keras/save_and_serialize

progress bar (seen in Ko):from tqdm import tqdm

ON THE ABOVE ERROR: i overrided the get_config in my models.py->class Conformer 

new error: 

4/4/2022

made a copy of current files that should work only with tf named model_tf200.py and keras_models_tf200.py

gpu low usage->>>>>>

install nvidia drivers again
run in ubuntu and if all fails try pytorch...or im killing myself

thema diplwmatikhs: pws h neural network proseggisi einai kalyterh apo sound 

CHANGED CUDA+PATH variable from 11.0 to 11.5 and on the Path to be on top

5/4/2022

TODO::: try ubuntu with gpu
 DID IT run quick test on cpu only 
 
TODO:::: see the difference in models.py in ubuntu and windows 
make a final program, also check the dimension problem, i think instead of (None, 256, 60, 2) it should be (None, 60, 512)((B, S, d_model))


7/4/2022
failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
cuda 11.5 to 11.0 from environmental variables

11/4/2022

cuda 10.0 and cudnn 7.6.5 to combat the gem blass issue(loook in issues.txt)
so i replaceed cuda 11.0 with this

14/4/2022

run resenet32 on ubuntu (pseudoresnet) auugmentation == 2 and trying to run same on resnet18

ResNet18 params when i run it no augmentation and 40 epochs(on vienna) = 4,788,016
PseudoResNet18 with augmentation == 1.5 million