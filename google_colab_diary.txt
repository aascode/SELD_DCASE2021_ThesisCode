18/4/2022
found github https://github.com/qubvel/classification_models
may try some implementations if i have time
it has ready resnets->mine may not be good
using the githubs resnet18 waiting for resutls

could not make it work>back to windows version

CHECK THIS:https://www.youtube.com/watch?v=ZUKz4125WNI
how to plot loss and do callbacks for ideal learning rate->>MIGHT INCLUDE THIS IN PAPER
will attempt to run simultaneously another google colab notebook
with conformer ubuntu but not the ready-conformer->the version that i run at 14 April on github!!(the  one i did when i played spooderman)
uploaded (on drive) seld2.py and keras_models2.py that calls the april 14 conformer version
also added to seld2 the checkpoints and the history plot code i added to the seld.py drive file(have not uploaded to github yet) 
Unfortunately can only run one notebook with gpu at once

changed the learning rate to 0.003 in keras_models.py in my drive for google colab resnet18 daugm=2
ALSO LOOK into changing frequncy and window hop->>next test

NOW RUNNING::->Turbox locally resnet18 da=2 (gpu)
		 ->Google Colab resnet18 da=2 lr=0.003 (cpu)
		 ->Google colab my conformer da=2 (gpu)
		 ->Lenovo ready conformer da=2 (gpu)

CANNOT run tf 2.0.0 with gpu in collab
tried with tf 2.2.0, keras 2.2.4, keras 2.3.0
tf 2.5.0 keras 2.5.0
tf 2.3.0 keras 2.3.1
noting worked
SO trying to run tf 2.4.1 keras 2.4.1 with gpu

19/4/2022
Google colab esnet18 takes ime in cpu and since i already run same model and aumentation locally on turbox, i stopped it
i will try to run same (da=2, lr=0.003) with baseline since it has fewer parameters
i will also try and put AdaBelief

NOW RUNNING::->Turbox locally resnet18 da=2 (gpu)
		 ->Google Colab baseline da=2 lr=0.003 AdaBelief (cpu)
		 ->Google colab my conformer da=2 (gpu) (stuck loss = 0.0384)
		 ->Lenovo ready conformer da=2 (gpu)

20/4/2022

Google Colab baseline da=2 lr=0.003 AdaBelief (cpu)->4 epochs

Getting Unkown layer: Conformer
to save and load the conformer custom class, i will save in tf format isntad of h5->didnt work
nvm it was in load_model needed to adda gain the {Conformer: Conformer} tag->it now works->> uloading to github

21/4/2022

NOW RUNNING::->Turbox locally baseline da=2 lr=0.003 AdaBelief(tf 2.4.1 keras 2.4.1) (gpu)->DONE with resnet18 gave better than no augm but the visualization was wors for some reason (on github, result_models/resnet18_da2)
		 ->Google Colab baseline da=2 lr=0.003 AdaBelief (cpu)->>will do it loally
		 ->Google colab my conformer da=2 (tf 2.0.0 keras 2.3.1) F 2.0.0 CANT RUN GPU (stuck loss = 0.0384)->continuing, might also run locally on LENOVO cause of colabs gpu restriction 
		 ->Lenovo ready conformer da=2 (gpu)

22/4/2022

Baseline da0 with AdaBelief gives trash ->not good
will try with Adam again but will give lr=0.003 da0

TO DO::::::::::
>Baseline takes only around 5 hours so i will experiment on it for:
 diferent opimizers, learning rates, window and frequency, custom learing rate decay, custom gradient training (look t Ko (with ... Gradient.tape))  ...
>ALSO try my conformer with turbox but NO AUGM and LSTM as decoder (read conformer paper)->conda with tf 2.1.0 keras 2.3.1 to not use the channels_last OR tf 2.4.1 keras 2.4.1 (turbox locally not conda)
>AND pseudoresnet34 no augm on lenovo to see how it was run (i think da2 and tf 2.4.1, but to make sure, if it is then my augmentation (offline) was not wrong)
>>ALSO try sepaating the sed and doa like KO!!

running tf 2.1.0 keras 2.2.4 does not fully utilize the gpu
and also constant result (0.0380) for baseline NO AUGMENTATION adam lr=0.003

will run evrithing on tf 2.4.1 and keras 2.4.1 even if im concerned with channels_last i need to run tf 2.4.1

TRY EVRITHING WITH 0 DATA AUGMENTATION
OR
TRY OFFLINE DATA AUGMENTATION->WROTE CODE IT ON LENOVO FOR SPECAUGM TOO

I think i run pseudoresnet34 (th one that worked) with tf 2.4.1 and da2 

NOW RUNNING::->Turbox locally baseline da=0 lr=0.003 Adam(tf 2.4.1 keras 2.4.1) (gpu)-> COMPUTER DECIDED TO TURN OF WHILE I WAS SLEEPING SO HAVE TO RESTART!!! YAYYYYYYY
		 ->Google colab my conformer da=2 (tf 2.0.0 keras 2.3.1) F 2.0.0 CANT RUN GPU (stuck loss = 0.0384)->continuing, might also run locally on LENOVO cause of colabs gpu restriction 
		 ->Lenovo resnet18 da=1 online(gpu)
		 ->Asus baseline da=0 Adam with binary cross entropy tf 2.4.1 keras 2.4.1

Tried online da=1 with resnet18 (lenovo still running tho), baseline (asus) and got trash results (negative loss)
maybe its my augmentation that breaks evrithing

FROM SCREENSHOTS in lenovo, i think the pseudoresnet34 that gave good results (uploaded on github on 13 April) was LSTM, da2 and tf 2.4.1 so channels_last not the problem

baseline da0 binary cross entropy also gives trash (negative loss)

rEAD PAPERS 

augmentation offline done on spectograms

https://www.google.com/search?q=d3net+architecture&sxsrf=APq-WBtzCMtSsUr-Xu3s4HZLjB2LkN2ArA%3A1650662178332&ei=IhtjYoL5E9X-sAeGtZHQCQ&oq=d3net+arc&gs_lcp=Cgdnd3Mtd2l6EAEYAzIFCCEQoAEyBQghEKABMgUIIRCgATIFCCEQoAEyBQghEKABOgcIIxCwAxAnOgcIABBHELADOgYIABAWEB5KBAhBGABKBAhGGABQjgtY7Rlg8C5oAnABeACAAYIBiAHDApIBAzIuMZgBAKABAcgBBcABAQ&sclient=gws-wiz#imgrc=X_p-FcC40wQzUM

23/4/2022

RUN::		 
		->Turbox locally baseline da=0 lr=0.1 Adam(tf 2.4.1 keras 2.4.1) (gpu)-> gave loss of 1.034 instead of 0.0380 and could not converge
		->Asus da=0 baseline Adam lr=0.0001
		->Turbox baseline AdaBelief but got trash negative loss
		 ->Lenovo pseudoresnet18 da=1 online(gpu)->RUN BUT process killed when dumping results->will do this on turbox then but results where again negative loss trash but not constant like the ready conformer
		 ->Lenovo ready_conformer da2 (gpu tf 2.4.1)->gave trash constant negative loss of -9.610e-4
		->Asus and Turbox baseline with sgd gave constant loss of 0.0379
		->pseudoresnet34 da2 tf 2.4.1
		->resnet18 da0 tf 2.0.0 
		

NOW RUNNING::->Turbox locally baseline da=0 Adam VANILLA(tf 2.4.1 keras 2.4.1) (gpu)
		 ->Lenovo my-conformer da=0 with lr=0.0001 Adam (gpu tf 2.4.1)
		 ->Asus baseline da=0 SGD (cpu)

BASLine gives normal results on tf 2.4.1 so channels_last really is no problem
Will do data augm = 1 on turbox now > siggline pio grigora stin arxh alla den eftase 0.74 opws me da0->akyro EGINE ME ACCDOA FALSE

24/4/2022

Calling my conformer fun (not class) on turbox, da0, weight regulizers from Ko and Adam 0.00002 loss
Also added extra dense 512 and Dense 32 layer like Zhang
 
For some reason, while tr_loss very low, the SELD score remains the worst (1.00) on adabelief baseline 

Lenovo run a couple of tests with ready cnformer, but still getstable loss 0f 0.0382 or a good loss but on epoch 2 it spirals down to negative trash

now running on lenovo ready conformer with learning rate scheduler

25/4/2022

Turbox->read Zhang and Huang from dcase 2020->they replace gru and use the conformer like an encoder
Same with Ko but he uses gru as  well on the DOA branch, but i train both sed and doa so i only have one accdoa branch

Will run on turbox my conformer, weight reg and adam with learning rate schedulerBUT will now replace the gru
I will also run a conformer approach but try the computation on the 60 axis instead of 512

RUNNING:: Turbox ready conformer adabelief 0.0001 scheduler without gru and da0 BUT changed last dimension 512 to 60
	    Lenovo ready conformer adabelief 0.0001 scheduler without gru da0 15 subepochs


27/4/2022

the result of the ready conformer adam 0.0001 wreg scheduler is actually the model named my_conformer_da0_adabelief0_0001_nogru_scheduer_wreg_extradenselayer_inverseshape3
i just named it wrong on run, but the parameters are that i mentioned, not htose in the name
same goes for models my_conformer_da0_adabelief0_0001_nogru_scheduer_wreg_extradenselayer_inverseshape and my_conformer_da0_adabelief0_0001_nogru_scheduer_wreg_extradenselayer_inverseshape2
also batch size 16

All confomer implementations where done with only 1 depth of laer conformer
i will try with 2
also no lstm or gru where used
da0

now running: Lenovo base da2+da1
		 Turbox conformer da0, depth 2 and gru
		 Asus if i fix swa
copied the code from seld and keras models from turbox to lenovo

tried to run different freq 16000 and window len 0.01 and 0.2 in asus and it crashed, cannot load ubuntu stuck in grub terminal

28/4/2022

run ready_conformer_depth_da0_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse
wich i gave the 256 dimension instead of the 60 and it worked...how??->gave best score so far (name: ready_conformer_depth_da0_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse)
will rty same for 512 and then optimizaion techniques->i had to stop the 512 since it didnt give better results, i am now trying with 512 and no gru(i forgot and had gru enabled)
Akyro then eixa allaksei tis diastaseis, opote ekane akoma gia 60 me 512 stin prwth diastash(model  ready_conformer_depth_da0_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_512.txt)
Try again for (60, 512)->it gave constant loss of 0.0379

NOW RUNNING:Turbox ready conformer with 512 dimension no gru-> gave constant
		Lenovo trying ensembling with res-50 and base->gave bad ressults

have to try ensembling with different head and dim_head on conformer
mights use 3 diff conformer models and a base-crnn for ensembling
also use da2 on conformer 256->did it but with dim24 and head4 (ready_head4_dim24_conformer_depth_da2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256) the model is called ready_conformer_depth_da2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256

NOW RUNNING:Turbox ready conformer with 256, head=4, dim=24 dimension no gru->GAVE WORSE RESULTS THAN MORE HEADS (ready_conformer_heads4_dim24_depth_da0_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256 )
		Lenovo trying ensembling with 3 different conformers, the 256 one, the 2256 with had=4, dim=24, and the 128 one

tried ensembling resnet34 but got error with bigru

maybe my augmentation is not right->it gives worse results with even the baseline ! ->>LOOK INTO IT OR TRY ONLINE

changed tp data augmentation randomshit to take channels=1
running 256 dim with da2 and randomshift not anged->still worse than da0 so i will change the randomshift to take the feat instead of the mel_spect like i did at first

30/4/2022

FOR FURTHER IMPROVING PERFORMANCE::
Try Kos SWA with conformer since ensembling not working out also adabelief also change threshold to 0.3 like nguyen
ALSO test-time augmentation in nguyen https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d

run da2 and da1 together, got wose results than da2 sketo (ready_conformer_da21....)

running conformer da1, swa, adabelief 0.003->gave worse results

2/5/2022

random magnitude augm
also tta (salsa)
TDA:::https://arxiv.org/pdf/2101.02919.pdf->We use ResNet to extract local shift-invariant
features. Then Conformer is adopted to learn both local and
global context representations. We call our acoustic model
ResNet-Conformer

running adabelief, da2, swa 128 mel bands->ready_conformer_depth_swa_myda2_adabelief0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256 not better results 64 bands (deleted accidentally, but had ER: 0.92 at epoch 9 and 0.75 score
ready_conformer_depth_swa_myda2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256->gave similar results to best attempt, which was same with da2 and no swa (ready_conformer_depth_myda2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256)
Will run same but with both augmentations

IMPORTANT:: All conformer implementations have gru, i fuqed up
now running the da3 ready_conformer_depth_swa_myda3_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256 that ACTUALLY has no gru (also mel-bands 128)

NO GRU GIVESCONSTANT RESULT->always use it i guess

ready_conformer_depth_swa_myda3_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256_128mel gives worse results than jus da1

den yparxei logikh se tipota

11/5/2022

turbox fixed
downloaded cuda 11.0 and cudnn 8.0.3 following nvidias guide site and the youtube video https://www.youtube.com/watch?v=OEFKlRSd8Ic&t=977s
downloaded cuda 11.1 because of SubProcess ended with return code: 4294967295 i had before (check the text file )

running:
 	-Lenovo: resnet34, da2, sedxyz and rate scheduler 0.5 gia na parw ta weights tou
	-turbox: da3 conformer and 16 layers of attention 128 mel->FINISHED, gave constanbt loss as always... may need to augment the epochs

need to run conformer no rnn!!->finished, constant loss for 30 epochs and ensembling methods tommorrow

Try concatenating both foa+mic

12/5/2022

RUNNING: Turbox: resnet-conformer no rnn, 16 attn 

changed the cnn extraction part of the conformer approach to be same as Zhangs
also commented the dense doa extraction part, since the dense is already implemented after conformer
also changed strides=(2,2) in res_conv18, according to the originbal paper https://arxiv.org/pdf/1512.03385.pdf->it gave constant loss (resnet34_conformer_gru_16attn and 16attn2 so changedd it to strides(1,1,) (resnet34_gru_conformer_16attn3))
also made resnet_2020 model, will use in resnete ensembling, from paper A Model Ensemble Approach for Sound Event Localization and Detection

also added axis=1 (and not =3) on the res_conv18 batchnorm(), because of " bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1" in https://github.com/keras-team/keras-applications/blob/master/keras_applications/densenet.py

swa_scheduler0_5_weightINitializer_sedxyz_baseline_da2 gave better results than simple vanilla

RUN this week:
separate low, mid and top freq baseline models
baseline swa da2
baseline da1+2 
swa scheduller 0.5 baseline weightInitializers SEDxyz da2 -> run untiil epoch 9->better than simple baseline
swa scheduler 0.5 resnet34 weightInitializers SEDxyz da2-> didnt save txt file, but did worse than resnet18
---turbox
several conformer 16attn models and 128 mels
->one with resnet and without gru, one with resnet and gru and one byitself and no gru

turbox turned off again, doing everything all over  (resnet34_conformer_gru_16attn31)

14/5/2022

Running:: -lenovo ready conformer weighhInit, gru, 64 mel, zhangs cnn, da2 adam 0.001
	    -turbox resnet34_conformer_gru_16attn31

will try adam 0.001 from now on insteaad of 0.0001

STILL TO DO:
ResNet34 da1+2
TTA with ACS
bDNN from Ko->segmentation of test data to more chunks and averaging the overlapping segments
ENSEMBLES: -ResNet-Conformer
	     -3 Conformers systems (ResnetConformer gru, ResnetConformer no gru, Conformer with Zhangs cnn and conformer with deeper layers (instead of 16attn do 128))
	     -Dense-Conformer

seld: tried to add tta technique for mic-> the tta that Nguyen, Park and Shimada mention uses the 16-rotation FOA technique, i used the 8-rotation MIC one

next: dynamic threshold, ensembling overlaping outputs

15/5/2022

turbox got OOM error (resnet34_conformer_gru_16attn31)->7 epochs remaining doing it again from loading model weights from resnet34_conformer_gru_16attn31 (now running resnet34_conformer_gru_16attn32)
Once finished, will run batch extraction for 64 and Dense+Conformer ensemble


adaptive thresholding strategies

DenseNet is an extention to Wide Residual Networks:https://github.com/titu1994/DenseNet

spatial augmentation in tta:https://arxiv.org/pdf/2101.02919.pdf
and https://dcase.community/documents/challenge2019/technical reports/DCASE2019 MazzonYasuda 93.pdf

"There are only a limited set of transformations that can
be applied to the audio channels in order to keep the spatial
responses of the MIC data unchanged. Specifically, channel
swapping is used for the MIC data and there are only eight
allowable transformations to obtain effective audio data and
the corresponding DOA representations. "

LENOVO RUN swa_scheduler0_5_weightINitializer_conformer_da2-> gave good results score 0.55
will turn off laptop for short time

Cannot run ensembles in lenovo

WILL RUN:
	-Lenovo resnet2020-conformer with Kos aggregation of overlappping predictions
	-Turbox ensemble of Resnet34+Conformer(with Zhangs cnn, the one that i run) and tta with ensemble_seld,py from Lenovo , will later try Kos post-processing overlapping aggregation

i changed swa to class form (in turbox), couldnt save weights in the train_end->changed nothing, now just commented it


16attn actually uses 24 attention blocks not 16
also all attn16 models have da3

changed layernorm axis from -1 to 1 in attention.py in turbox

RUNNING:-Turbox zhang conformer 24 attentions and sedxyz da1+2 tta->need to start ensembling, cause my models could not load
	  -Lenovo resnet2020_conformer da2

FOR TTA: might need to rotate back the predictions before ensembling them

turbox gives nan loss (not tta or parallel fault)->adam 0.00001 loss to counter it->axis=1 still
so now running:turbox zhang conformer 24 attentions wreg and sedxyz da1+2 tta adam 0.00001 axis=1 batch=64(instead of 16 which was for each turbox model ever run)

adam 0.00005 gave constant loss, running adam 0.00001->constant after a while....
running adam -> constant
adabelief 0_00005 also constant

turned axis=-1 again in conformer_tf and tta->conformer_zhang_wreg_attn24_da12_tta_adam0_0001
changed zhangs cnn part to take filters 64, 128 and 256->it took 64, 128 and 192->made it like lenovos
changed axis to 1 again and tta->conformer_zhang_wreg_attn24_da12_tta_adam0_0001_axis1
noticed data in has 5 channels????->looking into that maybe thats why i got all constant loss with or without sedxyz or tta

batch feature extraction for da1+2 and 64 mels instead of 128
i think i messed up with the mels in the params and the mels i had the extracted data thats why it concluded channels was 5 instead of 10

RUNNING:Turbox:conformer_zhang_wreg_attn24_da12_tta_adam0_0001_axis_1->zhang conformer sedxyz with 64 mels, tta, da1_2 axis=1 adam 0_00001
	  Lenovo:resnet2020_conformer da2, no tta or aggregation
turbox still getting similar results wtf->could be weight regularizers

made lr scheduler to half the lr after epoch 15

difference in lenovos zhang conformer i run and turbox's: turbox is sedxyz and da1+2 and will use tta

TOMMOROW: HAVE TO START ENSEMBLES

17/5/2022
2_conformer_zhang_wreg_attn24_da12_tta_adam0_0001_axis_1 stopped due to OOEM while i was sleeping
->continuing it for 5 more epochs from where it stopped

LENOVOV RUN resnet2020_conformer_gru_da2 for 25 epochs->better results on epoch 25 than resnet34_conformer(that run on turbox)->will run 10 more epochs
->RUN gave same score as baselines

TO DO:
Dynamic thresholds(look kos git for values [0.35, 0.35, 0.3, 0.4, 0.65, 0.6, 0.45, 0.55, 0.3, 0.3, 0.45, 0.3]  (in make_answers.py))(https://ir.lib.uth.gr/xmlui/bitstream/handle/11615/50539/19379.pdf?sequence=1&isAllowed=y)
acs da training
tta
dense_conformer
squeeze cnn (Najar-Alcazar report)
track-wise (EINV2) output format combats overlap of same event class(see https://arxiv.org/pdf/2203.10228.pdf and Nguyen)

'SELDnet has the limitation that it is unable to detect sound
events of the same type but with different locations [1]. Event
independent network (EIN) with track-wise output format was
proposed to tackle this problem [4, 10]. In EIN, there are
several event-independent tracks, which means the prediction
on each track can be of any event type. The number of tracks
needs to be pre-determined according to the maximum number
of overlapping events'

'To increase the diversity of model ensembles,
we replace convolution blocks with dense blocks
'

'Methods like TTA,
average or weighted ensemble [8, 14] cannot align predictions
from different tracks, which makes these methods inapplicable to the track-wise output format. We propose a novel
post-processing method named track-wise ensemble model.
'

' In dense
blocks, each layer is connected with all preceding layers to obtain additional inputs, and delivers its own feature maps to all
subsequent layers. This feed-forward structure can strengthen
forward propagation of features and back propagation of gradients'(https://arxiv.org/pdf/2203.10228.pdf)

cant load the conformer model for ensemble for some reason->gives axes dont match array error
running new conformer similar to the best one 'conformer_da1_2_adam0_0001_wreg_scheduler_depth_gru_144attn'

cannt load pseudoresnet either, even if i did thius in the past on lenovo-> look into that
running vanilla da1_2 to use for moc ensemble
ensembling technique works but gives worse results (tried to load vanilla 2 times):
Class-aware localization scores: Localization Error: 68.4, Localization Recall: 10.5
        Location-aware detection scores: Error rate: 0.95, F-score: 2.5
        SELD (early stopping metric): 0.80

tta->might need to change the per_file param to False like data_gen_train\

RUNNING:Turbox vanilla no da because i got bad results with da1_2->after that ensemble
	  Lenovo tta da2 dense_conformer dynamic thresholds-> per__file on data_gen_test changed to False->changed it back to true

cls_data_generator deleted the code for the 3 ensemble cnns (check github before todays date to get the ensemble version)
tta cant do TfmapRandomSwapMic because it gives a 7 channel feqat and label output, but i need a 10 channel one
->ill do the GccRandomSwapMic but with all 8 rotations and not random->will have 8 data_gen_tests for each rotation

RUNNING:Turbox vanilla no da because i got bad results with da1_2->after that ensemble
	  Lenovo tta da2 dense_conformer dynamic thresholds SWA (previous swa in lenovo was i believe not correct, like it wasnt implemented)

18/5/2022
saving code (seld.py, keras_models.py and parameters.py) in seagate hard drive in models folder from now on

keras_models.py was at fault for giving axes dont match error
i replaced with the lenovos keras_models.py to load resnet2020_conformer (see git from todays date to get turbox latest)
and it loaded the weights
also axis=-1 in conformer_tf
will run resnet2020_conformer + CNN ensemble-> it WORKED!!!!!  gave 0.56 score
run it second time and gave 0.7 score with no change.....->i put quick_test:True in params and it gave bad results
Need to run all with quick test False


if i give test split fold 5 instead of 6, it gives better results....

run sarath baseline and got the expected results->so sth wrong with my code... will relace my seld.py

also cannot load_model->gives GRU error

Run resnet2020_conformer da0 on the sarath baseline->gave 0.0379 for 4 epochs->changed lr to 0.0001, got similar results like lenovo

resnet2020_conformer da0 gave 2.0 point less than with da1_2 on epoch 10

sedxyz worse than non parallel->lenovo baseline no parallel gave 0.78 score on first epoch while parallel gave 1.0

changed res_conv18 and res_identity18 in lenovo to match main githubs resnet18 da2 ( git timestamp a4a64c5e3e)->still cant load pseudoresnet->gives axes error

cant load pseudoresnet sadly, will run it again

RUNNING:: Lenovo ResNet18 da2, no tta

Changed seled.py in turbox with lenovos latest version with dyn thresholds
did all 8 rotations in data_augmentations for gcc

i think resnet2020_conformer got corrupted or sth cause it gives really bad results... will run it again

resnet2020_conformer no tta: 
resnet 2020_conformer tta (2/8 transformations)

RUNNING:: Lenovo ResNet18 da2, no tta and deleted the Dense(12, softmax) after the resnet18, because i believe it worsened the performance (resnet18_da2)->might run this with resnet34 as well (it had Dense(12, softmax) too)
          Turbox resnet2020_conformer da1_2 tta 8/8 transforms to get the trained weights back (resnet2020_conformer_tta2_acs2)

TO RUN::: Baseline
	    Dense_conformer
	    Squeeze_conformer
	    Resnet_conformer + Dense_Conformer + Squeeze_conformer
	    3 diff conformer models + Resnet2020
	    da1_2_3->ACS

resnet2020_conformer_da1_2_6epochs_tta8 and dynamic thresh->worse with tta 8/8???-> im idiot, didnt add the rotation back for all tta
same with 2/8 tta:
Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 30.2, Localization Recall: 30.8
        Location-aware detection scores: Error rate: 0.78, F-score: 20.9
        SELD (early stopping metric): 0.61

	second run:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 30.7, Localization Recall: 31.3
        Location-aware detection scores: Error rate: 0.78, F-score: 20.8
        SELD (early stopping metric): 0.61


Same with 0/8 (no tta):Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 37.9, Localization Recall: 36.4
        Location-aware detection scores: Error rate: 0.78, F-score: 18.5
        SELD (early stopping metric): 0.61

	second run with no tta:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 39.4, Localization Recall: 36.1
        Location-aware detection scores: Error rate: 0.79, F-score: 17.5
        SELD (early stopping metric): 0.62

TTA gives better LE but worse LR! also slightly better F-score

moved latest version of turbox (with acs and denseconv) to seagate

19/5/2022
resnet2020_conformer_tta2_acs2 done  (also has dynthreshold)
acs and tta0:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 37.7, Localization Recall: 44.8
        Location-aware detection scores: Error rate: 0.78, F-score: 21.3
        SELD (early stopping metric): 0.58

acs and tta2:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 31.6, Localization Recall: 41.6
        Location-aware detection scores: Error rate: 0.73, F-score: 25.8
        SELD (early stopping metric): 0.56

Since the model has also been trained with acs, it is now generally better with tta!

acs tta8:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 23.4, Localization Recall: 17.9
        Location-aware detection scores: Error rate: 0.84, F-score: 17.6
        SELD (early stopping metric): 0.65
2022-05-19 15:51:17.882068: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.

because it wasnt trained

RUNNING: Lenovo resnet18 again (resnet18_da2)->running for 35 epochs total
	   Turbox Resnet 34 tta acs da1_2_3 dyn threshold (resnet34_tta2_acs_da1_22)->run for 12 epochs
		NOW in turbox i run resnet34-conformer tta acs da1_2_3 dyn threshold(2_new_resnet34_conforer_tta2_da1_2_3_acs1_dyn_mic_dev_split6_model)

https://github.com/IRIS-AUDIO/SELD/blob/8be19266479f32659e2c1d9be7dd6a377fe7abed/modules.py#L452->has densenet, xception...


20/5/2022

RUn 2_new_resnet34_conforer_tta2_da1_2_3_acs1_dyn_mic_dev_split6_model for 20 epochs
    2_renet34_tta2_da1_2_acs for 12 epochs
    2_resnet18_da2 for 35 epochs

CNN+res2020_conformer+new_res34_conformer ensemble:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 27.8, Localization Recall: 38.0
        Location-aware detection scores: Error rate: 0.74, F-score: 26.2
        SELD (early stopping metric): 0.56
with tta2:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 24.4, Localization Recall: 35.1
        Location-aware detection scores: Error rate: 0.73, F-score: 28.6
        SELD (early stopping metric): 0.56

batch_extraction for 128 mel to load some conformer models
->try also diff window size->FIRS run ensembles

ensembling:cnn+res2020_conf+new_res34_conf+2_ready_conformer_depth_myda2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256
with tta2:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 23.6, Localization Recall: 29.9
        Location-aware detection scores: Error rate: 0.76, F-score: 25.7
        SELD (early stopping metric): 0.58
without:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 26.9, Localization Recall: 33.6
        Location-aware detection scores: Error rate: 0.76, F-score: 24.6
        SELD (early stopping metric): 0.58


baseline da2 sedxyz worse than baseline da2
conformer_swa_da1_2_3_tta2_dyn run



22/5/2022

'They found out that CNNs using waveform input can be competitive with CNNs using mel-spectrogram input when the network
layer is sufficiently deep'
'. However, they did not focused on the effect
of the small size filters or used only two or three convolution layers which is not sufficient to learn the complex acoustic
structures'

'the excitation is
associated with the loudness of audio and the activations of feature maps. This confirms that the excitation is associated with
discriminating different categories of audio'

'Mel-spectrogram is a widely used input representation for
audio classification. As shown in Fig. 1, they are obtained via
multiple computational steps including Short-Time Fourier
Transform (STFT), absolute value operation, linear-to-mel
mapping and magnitude compression.'
-Comparison and Analysis of SampleCNN
Architectures for Audio Classification

SampleCNN-> small filters and more layers (9 layers and 2 or 3 sized filters)

dense da1_2_3 tta2 run->0.55 score

RUNNING: Continuing resnet34
	   Dense conformer

23/5/2022

Dense conformer finished!!
running new_resnet34_conforer_da1_2_3_tta2_dyn
Also finishing resnet34->actually didnt use acs in lenovo-> i forgot to put tta=1 in data_gen_train and results are for tta false cause i got error
->running last 10 epochs with tta and acs
cant run acs due to memory leak in lenovo

exw kanei malakia me to swa thelei pollapla modela-> to vazw sto ensemble_seld.py
continuing the resnet2020_conformer_da1_2_3_tta2 model to 40 epochs (was 25)-> didnt ggive better results than at 25 epochs
try ensembles
try no gru again with new resnet-conformer->with new conformer gives constant loss
try conformer without the 3 dense layers

RUNNING::Lenovo resnet34_da2 acs andtta2 on last 10 epochs
	   Turbox alcajar squeeze conformer da1_2_3 tta4!! without the 2 dense layers

24/5/2022

Renset34_da1_2_3_tta2_da2_lenovo run->got 0.50 score!!
Conformer without dense and squeeze gives 0.51 in 10 epochs!!!!!!!

RUNNING::Lenovo squeeze alcazar cnn->OOM->running conformer_da2_no2dense now on 30th epoch->DONE gave no better results than squeeze_conformer-nodense
	   Turbox alcajar squeeze conformer da1_2_3 tta4!! without the 2 dense layers (NO GRU->constant)->DONE


25/5/2022
not much
9 epochs on base_128

26/5/2022

densenet+squeezeconformer:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 24.5, Localization Recall: 48.5
        Location-aware detection scores: Error rate: 0.65, F-score: 38.4
        SELD (early stopping metric): 0.48

BETTER LE

densenet+squeezeconformer+resnet34_da1_2_3_tta2(resnet 0.55 score before lenovo, 12 epochs version):
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 23.2, Localization Recall: 44.2
        Location-aware detection scores: Error rate: 0.67, F-score: 36.8
        SELD (early stopping metric): 0.50


Ensemble 1 (2 ResConformers, 2 Conformers):
	Resnet34Conformer + ResNetlesslayersConformer + Conformer128mel + SqueezeConformer:
		
Ensemble 2 (1 CNN, 2 Conformers, 1 ResNet) (DenseNet, SqueezeConformer+ '2_ready_conformer_depth_myda2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256_mic_dev_split6_model, resnet34 12 epochs):
	Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 22.4, Localization Recall: 38.7
        Location-aware detection scores: Error rate: 0.70, F-score: 33.6
        SELD (early stopping metric): 0.53

Ensemble 3 (1 Resnet-conformer, 2 Conformers, 1 resnet) (2_new_resnet34_conforer_tta2_da1_2_3_acs1_dyn , (squeeze) 2_conformer_da1_2_3_tta4_no2dense+2_conformer_swa_da1_2_3_tta2_dyn, 2_resnet34_tta2_acs_da1_2):
	Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 23.4, Localization Recall: 42.7
        Location-aware detection scores: Error rate: 0.68, F-score: 35.9
        SELD (early stopping metric): 0.51


LOADED model2 that was the conformer da1_2_3 with densenet-conformer model layers...
Ensemble 4 (1 CNN, 1 Resnet-conformer, 2 Conformers, 1 resnet) (DenseNet, same asa above):Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 22.7, Localization Recall: 42.1
        Location-aware detection scores: Error rate: 0.68, F-score: 35.9
        SELD (early stopping metric): 0.51
	(SIMILAR TO ABOVE, better LE)

Ensemble 5 (1 Resnet-conformer, 3 Conformers, 1 resnet)(same as above + densenet_conformer BUT resnet34 full epochs):
	Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 22.8, Localization Recall: 40.3
        Location-aware detection scores: Error rate: 0.69, F-score: 35.1
        SELD (early stopping metric): 0.52

To idio gia 3 fores to Squeeze Conformer:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 23.9, Localization Recall: 45.9
        Location-aware detection scores: Error rate: 0.66, F-score: 38.4
        SELD (early stopping metric): 0.49

Ensemble 6 (1 resnet + 3 conformers):
	Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 23.7, Localization Recall: 44.1
        Location-aware detection scores: Error rate: 0.68, F-score: 36.6
        SELD (early stopping metric): 0.50
Ensemble ? (SqueezeConformer + Resnet34):Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 24.4, Localization Recall: 43.9
        Location-aware detection scores: Error rate: 0.68, F-score: 36.3
        SELD (early stopping metric): 0.50
Ensemble ?? (same as above with DesneNet cnn):Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 24.9, Localization Recall: 48.7
        Location-aware detection scores: Error rate: 0.65, F-score: 39.3
        SELD (early stopping metric): 0.48
Ensemble 7 (best of all models):
	Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 23.1, Localization Recall: 45.8
        Location-aware detection scores: Error rate: 0.66, F-score: 38.5
        SELD (early stopping metric): 0.49
Ενσεμβλε 8: 2 ΡεσνετΨονφορμερ + 2 Conformers:Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 24.4, Localization Recall: 44.3
        Location-aware detection scores: Error rate: 0.68, F-score: 35.7
        SELD (early stopping metric): 0.50

Ensemble 9: 3 conformer, 1 cnn:
Results on test split:
        DCASE2021 Scores
        Class-aware localization scores: Localization Error: 23.1, Localization Recall: 43.9
        Location-aware detection scores: Error rate: 0.67, F-score: 37.4
        SELD (early stopping metric): 0.50

Ensemble 11 : Ensemble ?? +  densenet+squeezeconformer