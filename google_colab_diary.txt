18/4/2022
found github https://github.com/qubvel/classification_models
may try some implementations if i have time
it has ready resnets->mine may not be good
using the githubs resnet18 waiting for resutls

could not make it work>back to windows version

CHECK THIS:https://www.youtube.com/watch?v=ZUKz4125WNI
how to plot loss and do callbacks for ideal learning rate->>MIGHT INCLUDE THIS IN PAPER
will attempt to run simultaneously another google colab notebook
with conformer ubuntu but not the ready-conformer->the version that i run at 14 April on github!!(the  one i did when i played spooderman)
uploaded (on drive) seld2.py and keras_models2.py that calls the april 14 conformer version
also added to seld2 the checkpoints and the history plot code i added to the seld.py drive file(have not uploaded to github yet) 
Unfortunately can only run one notebook with gpu at once

changed the learning rate to 0.003 in keras_models.py in my drive for google colab resnet18 daugm=2
ALSO LOOK into changing frequncy and window hop->>next test

NOW RUNNING::->Turbox locally resnet18 da=2 (gpu)
		 ->Google Colab resnet18 da=2 lr=0.003 (cpu)
		 ->Google colab my conformer da=2 (gpu)
		 ->Lenovo ready conformer da=2 (gpu)

CANNOT run tf 2.0.0 with gpu in collab
tried with tf 2.2.0, keras 2.2.4, keras 2.3.0
tf 2.5.0 keras 2.5.0
tf 2.3.0 keras 2.3.1
noting worked
SO trying to run tf 2.4.1 keras 2.4.1 with gpu

19/4/2022
Google colab esnet18 takes ime in cpu and since i already run same model and aumentation locally on turbox, i stopped it
i will try to run same (da=2, lr=0.003) with baseline since it has fewer parameters
i will also try and put AdaBelief

NOW RUNNING::->Turbox locally resnet18 da=2 (gpu)
		 ->Google Colab baseline da=2 lr=0.003 AdaBelief (cpu)
		 ->Google colab my conformer da=2 (gpu) (stuck loss = 0.0384)
		 ->Lenovo ready conformer da=2 (gpu)

20/4/2022

Google Colab baseline da=2 lr=0.003 AdaBelief (cpu)->4 epochs

Getting Unkown layer: Conformer
to save and load the conformer custom class, i will save in tf format isntad of h5->didnt work
nvm it was in load_model needed to adda gain the {Conformer: Conformer} tag->it now works->> uloading to github

21/4/2022

NOW RUNNING::->Turbox locally baseline da=2 lr=0.003 AdaBelief(tf 2.4.1 keras 2.4.1) (gpu)->DONE with resnet18 gave better than no augm but the visualization was wors for some reason (on github, result_models/resnet18_da2)
		 ->Google Colab baseline da=2 lr=0.003 AdaBelief (cpu)->>will do it loally
		 ->Google colab my conformer da=2 (tf 2.0.0 keras 2.3.1) F 2.0.0 CANT RUN GPU (stuck loss = 0.0384)->continuing, might also run locally on LENOVO cause of colabs gpu restriction 
		 ->Lenovo ready conformer da=2 (gpu)

22/4/2022

Baseline da0 with AdaBelief gives trash ->not good
will try with Adam again but will give lr=0.003 da0

TO DO::::::::::
>Baseline takes only around 5 hours so i will experiment on it for:
 diferent opimizers, learning rates, window and frequency, custom learing rate decay, custom gradient training (look t Ko (with ... Gradient.tape))  ...
>ALSO try my conformer with turbox but NO AUGM and LSTM as decoder (read conformer paper)->conda with tf 2.1.0 keras 2.3.1 to not use the channels_last OR tf 2.4.1 keras 2.4.1 (turbox locally not conda)
>AND pseudoresnet34 no augm on lenovo to see how it was run (i think da2 and tf 2.4.1, but to make sure, if it is then my augmentation (offline) was not wrong)
>>ALSO try sepaating the sed and doa like KO!!

running tf 2.1.0 keras 2.2.4 does not fully utilize the gpu
and also constant result (0.0380) for baseline NO AUGMENTATION adam lr=0.003

will run evrithing on tf 2.4.1 and keras 2.4.1 even if im concerned with channels_last i need to run tf 2.4.1

TRY EVRITHING WITH 0 DATA AUGMENTATION
OR
TRY OFFLINE DATA AUGMENTATION->WROTE CODE IT ON LENOVO FOR SPECAUGM TOO

I think i run pseudoresnet34 (th one that worked) with tf 2.4.1 and da2 

NOW RUNNING::->Turbox locally baseline da=0 lr=0.003 Adam(tf 2.4.1 keras 2.4.1) (gpu)-> COMPUTER DECIDED TO TURN OF WHILE I WAS SLEEPING SO HAVE TO RESTART!!! YAYYYYYYY
		 ->Google colab my conformer da=2 (tf 2.0.0 keras 2.3.1) F 2.0.0 CANT RUN GPU (stuck loss = 0.0384)->continuing, might also run locally on LENOVO cause of colabs gpu restriction 
		 ->Lenovo resnet18 da=1 online(gpu)
		 ->Asus baseline da=0 Adam with binary cross entropy tf 2.4.1 keras 2.4.1

Tried online da=1 with resnet18 (lenovo still running tho), baseline (asus) and got trash results (negative loss)
maybe its my augmentation that breaks evrithing

FROM SCREENSHOTS in lenovo, i think the pseudoresnet34 that gave good results (uploaded on github on 13 April) was LSTM, da2 and tf 2.4.1 so channels_last not the problem

baseline da0 binary cross entropy also gives trash (negative loss)

rEAD PAPERS 

augmentation offline done on spectograms

https://www.google.com/search?q=d3net+architecture&sxsrf=APq-WBtzCMtSsUr-Xu3s4HZLjB2LkN2ArA%3A1650662178332&ei=IhtjYoL5E9X-sAeGtZHQCQ&oq=d3net+arc&gs_lcp=Cgdnd3Mtd2l6EAEYAzIFCCEQoAEyBQghEKABMgUIIRCgATIFCCEQoAEyBQghEKABOgcIIxCwAxAnOgcIABBHELADOgYIABAWEB5KBAhBGABKBAhGGABQjgtY7Rlg8C5oAnABeACAAYIBiAHDApIBAzIuMZgBAKABAcgBBcABAQ&sclient=gws-wiz#imgrc=X_p-FcC40wQzUM

23/4/2022

RUN::		 
		->Turbox locally baseline da=0 lr=0.1 Adam(tf 2.4.1 keras 2.4.1) (gpu)-> gave loss of 1.034 instead of 0.0380 and could not converge
		->Asus da=0 baseline Adam lr=0.0001
		->Turbox baseline AdaBelief but got trash negative loss
		 ->Lenovo pseudoresnet18 da=1 online(gpu)->RUN BUT process killed when dumping results->will do this on turbox then but results where again negative loss trash but not constant like the ready conformer
		 ->Lenovo ready_conformer da2 (gpu tf 2.4.1)->gave trash constant negative loss of -9.610e-4
		->Asus and Turbox baseline with sgd gave constant loss of 0.0379
		->pseudoresnet34 da2 tf 2.4.1
		->resnet18 da0 tf 2.0.0 
		

NOW RUNNING::->Turbox locally baseline da=0 Adam VANILLA(tf 2.4.1 keras 2.4.1) (gpu)
		 ->Lenovo my-conformer da=0 with lr=0.0001 Adam (gpu tf 2.4.1)
		 ->Asus baseline da=0 SGD (cpu)

BASLine gives normal results on tf 2.4.1 so channels_last really is no problem
Will do data augm = 1 on turbox now > siggline pio grigora stin arxh alla den eftase 0.74 opws me da0->akyro EGINE ME ACCDOA FALSE

24/4/2022

Calling my conformer fun (not class) on turbox, da0, weight regulizers from Ko and Adam 0.00002 loss
Also added extra dense 512 and Dense 32 layer like Zhang
 
For some reason, while tr_loss very low, the SELD score remains the worst (1.00) on adabelief baseline 

Lenovo run a couple of tests with ready cnformer, but still getstable loss 0f 0.0382 or a good loss but on epoch 2 it spirals down to negative trash

now running on lenovo ready conformer with learning rate scheduler

25/4/2022

Turbox->read Zhang and Huang from dcase 2020->they replace gru and use the conformer like an encoder
Same with Ko but he uses gru as  well on the DOA branch, but i train both sed and doa so i only have one accdoa branch

Will run on turbox my conformer, weight reg and adam with learning rate schedulerBUT will now replace the gru
I will also run a conformer approach but try the computation on the 60 axis instead of 512

RUNNING:: Turbox ready conformer adabelief 0.0001 scheduler without gru and da0 BUT changed last dimension 512 to 60
	    Lenovo ready conformer adabelief 0.0001 scheduler without gru da0 15 subepochs


27/4/2022

the result of the ready conformer adam 0.0001 wreg scheduler is actually the model named my_conformer_da0_adabelief0_0001_nogru_scheduer_wreg_extradenselayer_inverseshape3
i just named it wrong on run, but the parameters are that i mentioned, not htose in the name
same goes for models my_conformer_da0_adabelief0_0001_nogru_scheduer_wreg_extradenselayer_inverseshape and my_conformer_da0_adabelief0_0001_nogru_scheduer_wreg_extradenselayer_inverseshape2
also batch size 16

All confomer implementations where done with only 1 depth of laer conformer
i will try with 2
also no lstm or gru where used
da0

now running: Lenovo base da2+da1
		 Turbox conformer da0, depth 2 and gru
		 Asus if i fix swa
copied the code from seld and keras models from turbox to lenovo

tried to run different freq 16000 and window len 0.01 and 0.2 in asus and it crashed, cannot load ubuntu stuck in grub terminal

28/4/2022

run ready_conformer_depth_da0_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse
wich i gave the 256 dimension instead of the 60 and it worked...how??->gave best score so far (name: ready_conformer_depth_da0_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse)
will rty same for 512 and then optimizaion techniques->i had to stop the 512 since it didnt give better results, i am now trying with 512 and no gru(i forgot and had gru enabled)
Akyro then eixa allaksei tis diastaseis, opote ekane akoma gia 60 me 512 stin prwth diastash(model  ready_conformer_depth_da0_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_512.txt)
Try again for (60, 512)->it gave constant loss of 0.0379

NOW RUNNING:Turbox ready conformer with 512 dimension no gru-> gave constant
		Lenovo trying ensembling with res-50 and base->gave bad ressults

have to try ensembling with different head and dim_head on conformer
mights use 3 diff conformer models and a base-crnn for ensembling
also use da2 on conformer 256->did it but with dim24 and head4 (ready_head4_dim24_conformer_depth_da2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256) the model is called ready_conformer_depth_da2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256

NOW RUNNING:Turbox ready conformer with 256, head=4, dim=24 dimension no gru->GAVE WORSE RESULTS THAN MORE HEADS (ready_conformer_heads4_dim24_depth_da0_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256 )
		Lenovo trying ensembling with 3 different conformers, the 256 one, the 2256 with had=4, dim=24, and the 128 one

tried ensembling resnet34 but got error with bigru

maybe my augmentation is not right->it gives worse results with even the baseline ! ->>LOOK INTO IT OR TRY ONLINE

changed tp data augmentation randomshit to take channels=1
running 256 dim with da2 and randomshift not anged->still worse than da0 so i will change the randomshift to take the feat instead of the mel_spect like i did at first

30/4/2022

FOR FURTHER IMPROVING PERFORMANCE::
Try Kos SWA with conformer since ensembling not working out also adabelief also change threshold to 0.3 like nguyen
ALSO test-time augmentation in nguyen https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d

run da2 and da1 together, got wose results than da2 sketo (ready_conformer_da21....)

running conformer da1, swa, adabelief 0.003->gave worse results

2/5/2022

random magnitude augm
also tta (salsa)
TDA:::https://arxiv.org/pdf/2101.02919.pdf->We use ResNet to extract local shift-invariant
features. Then Conformer is adopted to learn both local and
global context representations. We call our acoustic model
ResNet-Conformer

running adabelief, da2, swa 128 mel bands->ready_conformer_depth_swa_myda2_adabelief0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256 not better results 64 bands (deleted accidentally, but had ER: 0.92 at epoch 9 and 0.75 score
ready_conformer_depth_swa_myda2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256->gave similar results to best attempt, which was same with da2 and no swa (ready_conformer_depth_myda2_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256)
Will run same but with both augmentations

IMPORTANT:: All conformer implementations have gru, i fuqed up
now running the da3 ready_conformer_depth_swa_myda3_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256 that ACTUALLY has no gru (also mel-bands 128)

NO GRU GIVESCONSTANT RESULT->always use it i guess

ready_conformer_depth_swa_myda3_adam0_0_0001_nogru_scheduler_wreg_extradenselayer_noinverse_256_128mel gives worse results than jus da1

den yparxei logikh se tipota

11/5/2022

turbox fixed
downloaded cuda 11.0 and cudnn 8.0.3 following nvidias guide site and the youtube video https://www.youtube.com/watch?v=OEFKlRSd8Ic&t=977s
downloaded cuda 11.1 because of SubProcess ended with return code: 4294967295 i had before (check the text file )

running:
 	-Lenovo: resnet34, da2, sedxyz and rate scheduler 0.5 gia na parw ta weights tou
	-turbox: da3 conformer and 16 layers of attention 128 mel->FINISHED, gave constanbt loss as always... may need to augment the epochs

need to run conformer no rnn!!->finished, constant loss for 30 epochs and ensembling methods tommorrow

Try concatenating both foa+mic

12/5/2022

RUNNING: Turbox: resnet-conformer no rnn, 16 attn 

changed the cnn extraction part of the conformer approach to be same as Zhangs
also commented the dense doa extraction part, since the dense is already implemented after conformer
also changed strides=(2,2) in res_conv18, according to the originbal paper https://arxiv.org/pdf/1512.03385.pdf->it gave constant loss (resnet34_conformer_gru_16attn and 16attn2 so changedd it to strides(1,1,) (resnet34_gru_conformer_16attn3))
also made resnet_2020 model, will use in resnete ensembling, from paper A Model Ensemble Approach for Sound Event Localization and Detection

also added axis=1 (and not =3) on the res_conv18 batchnorm(), because of " bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1" in https://github.com/keras-team/keras-applications/blob/master/keras_applications/densenet.py

swa_scheduler0_5_weightINitializer_sedxyz_baseline_da2 gave better results than simple vanilla

RUN this week:
separate low, mid and top freq baseline models
baseline swa da2
baseline da1+2 
swa scheduller 0.5 baseline weightInitializers SEDxyz da2 -> run untiil epoch 9->better than simple baseline
swa scheduler 0.5 resnet34 weightInitializers SEDxyz da2-> didnt save txt file, but did worse than resnet18
---turbox
several conformer 16attn models and 128 mels
->one with resnet and without gru, one with resnet and gru and one byitself and no gru

turbox turned off again, doing everything all over  (resnet34_conformer_gru_16attn31)

14/5/2022

Running:: -lenovo ready conformer weighhInit, gru, 64 mel, zhangs cnn, da2 adam 0.001
	    -turbox resnet34_conformer_gru_16attn31

will try adam 0.001 from now on insteaad of 0.0001

STILL TO DO:
ResNet34 da1+2
TTA with ACS
bDNN from Ko->segmentation of test data to more chunks and averaging the overlapping segments
ENSEMBLES: -ResNet-Conformer
	     -3 Conformers systems (ResnetConformer gru, ResnetConformer no gru, Conformer with Zhangs cnn and conformer with deeper layers (instead of 16attn do 128))
	     -Dense-Conformer

seld: tried to add tta technique for mic-> the tta that Nguyen, Park and Shimada mention uses the 16-rotation FOA technique, i used the 8-rotation MIC one

next: dynamic threshold, ensembling overlaping outputs

15/5/2022

turbox got OOM error (resnet34_conformer_gru_16attn31)->7 epochs remaining doing it again from loading model weights from resnet34_conformer_gru_16attn31 (now running resnet34_conformer_gru_16attn32)
Once finished, will run batch extraction for 64 and Dense+Conformer ensemble


adaptive thresholding strategies

DenseNet is an extention to Wide Residual Networks:https://github.com/titu1994/DenseNet

spatial augmentation in tta:https://arxiv.org/pdf/2101.02919.pdf
and https://dcase.community/documents/challenge2019/technical reports/DCASE2019 MazzonYasuda 93.pdf

"There are only a limited set of transformations that can
be applied to the audio channels in order to keep the spatial
responses of the MIC data unchanged. Specifically, channel
swapping is used for the MIC data and there are only eight
allowable transformations to obtain effective audio data and
the corresponding DOA representations. "

LENOVO RUN swa_scheduler0_5_weightINitializer_conformer_da2-> gave good results score 0.55
will turn off laptop for short time

Cannot run ensembles in lenovo

WILL RUN:
	-Lenovo resnet2020-conformer with Kos aggregation of overlappping predictions
	-Turbox ensemble of Resnet34+Conformer(with Zhangs cnn, the one that i run) and tta with ensemble_seld,py from Lenovo , will later try Kos post-processing overlapping aggregation

i changed swa to class form (in turbox), couldnt save weights in the train_end->changed nothing, now just commented it